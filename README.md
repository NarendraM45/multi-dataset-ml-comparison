# ğŸ§  Multi-Dataset Machine Learning Comparative Analysis

A comprehensive machine learning project analyzing the performance of multiple algorithms across **16 diverse datasets**, covering **Classification**, **Regression**, and **Clustering** tasks.  
This repository provides a unified framework to evaluate, visualize, and interpret how different models behave on datasets of varying complexity and data types.

---

## ğŸ“˜ Project Overview

The goal of this project is to **compare algorithmic performance** on real-world and synthetic datasets using consistent preprocessing, hyperparameters, and evaluation metrics.  
The results highlight how certain models generalize better across different learning paradigms.

This study combines:
- ğŸ§© **Classification Models:** Logistic Regression, Random Forest, AdaBoost, SVM, KNN, Naive Bayes  
- ğŸ“ˆ **Regression Models:** Linear Regression, SVR, Random Forest Regressor, Bagging Regressor  
- ğŸ” **Clustering Models:** K-Means, Agglomerative Clustering  

---

## ğŸ§  Datasets Used

| **#** | **Dataset Name** | **Type** | **Source / Description** |
|:--:|:----------------------|:----------------|:----------------|
| 1 | **Iris** | Classification | Flower classification â€” built-in (Scikit-learn) |
| 2 | **Wine** | Classification | Chemical analysis dataset â€” built-in |
| 3 | **Breast Cancer** | Classification | Tumor diagnostic dataset â€” built-in |
| 4 | **Digits** | Classification | Handwritten digits (0â€“9) â€” built-in |
| 5 | **Titanic** | Classification | Passenger survival prediction dataset |
| 6 | **Adult Income** | Classification | Predict income > \$50K/year â€” UCI dataset |
| 7 | **Credit Card Fraud** | Classification | Imbalanced fraud detection dataset â€” [Download manually](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) |
| 8 | **MNIST** | Classification | Image-based digit dataset â€” Keras/Scikit-learn |
| 9 | **IMDB** | Classification | Sentiment classification for movie reviews |
| 10 | **Diabetes** | Regression | Predict disease progression â€” built-in |
| 11 | **California Housing** | Regression | Predict housing prices â€” built-in |
| 12 | **Linnerud** | Regression | Multi-output regression dataset â€” built-in |
| 13 | **make_classification** | Synthetic Classification | Generated with `sklearn.datasets.make_classification()` |
| 14 | **make_moons** | Synthetic Classification | Non-linear 2D dataset for testing kernel methods |
| 15 | **make_circles** | Synthetic Classification | Concentric circular dataset â€” kernel-based models |
| 16 | **make_blobs** | Clustering | Synthetic blobs for unsupervised clustering (K-Means, Agglomerative) |

> âš ï¸ The `creditcard.csv` dataset is **too large to include in this repository**.  
> Please download it manually from Kaggle and place it in your project folder before running the script.

---

## ğŸ§° Models & Algorithms

| **Type** | **Models Used** |
|-----------|----------------|
| Classification | Logistic Regression, SVM (RBF), Decision Tree, Random Forest, AdaBoost, GaussianNB, MultinomialNB, KNN |
| Regression | Linear Regression, SVR, Decision Tree Regressor, Random Forest Regressor, Bagging Regressor |
| Clustering | K-Means, Agglomerative Clustering |

Each model was evaluated using the following metrics:
- **Classification:** Accuracy, F1-Score, Precision, Recall  
- **Regression:** Mean Squared Error (MSE)  
- **Clustering:** Silhouette Score  

---

## ğŸ“Š Results Overview

The experiments revealed several insights:

- **Random Forest** and **AdaBoost** consistently achieved top accuracy across most classification datasets.  
- **SVM (RBF)** excelled in handling non-linear patterns (e.g., `make_moons`, `make_circles`).  
- **KNN** performed effectively on small, well-separated datasets like Iris and Wine.  
- **Linear Regression** was efficient for low-dimensional regression problems like Diabetes.  
- **Random Forest Regressor** and **Bagging Regressor** minimized MSE for complex datasets such as California Housing.  
- For **Clustering**, both **K-Means** and **Agglomerative Clustering** achieved strong silhouette scores on `make_blobs`.

---

## ğŸ“‚ Repository Structure
```
multi-dataset-ml-comparison/
â”‚
â”œâ”€â”€ plots/ # All generated charts and evaluation visuals
â”‚ â”œâ”€â”€ classification_accuracy_boxplot.png
â”‚ â”œâ”€â”€ classification_accuracy_heatmap.png
â”‚ â”œâ”€â”€ classification_avg_accuracy.png
â”‚ â”œâ”€â”€ classification_f1score_comparison.png
â”‚ â”œâ”€â”€ clustering_silhouette_comparison.png
â”‚ â”œâ”€â”€ regression_mse_boxplot.png
â”‚ â”œâ”€â”€ regression_mse_comparison.png
â”‚ â”œâ”€â”€ model_performance_summary.csv
â”‚ â””â”€â”€ top_models_summary.csv
â”‚
â”œâ”€â”€ mulitidaatset_compariosn.py # Main Python script for model training and evaluation
â”œâ”€â”€ visual.ipynb # Jupyter notebook for visualization and insights
â”œâ”€â”€ visual.pdf # Exported PDF report of visualizations
â”œâ”€â”€ ml_results_full_2.csv # Master table of model results
â”œâ”€â”€ ml_results_full_2.xlsx # Excel version of results
â”œâ”€â”€ ml_results_full_2.html # Interactive HTML results summary
â”œâ”€â”€ top_models_summary.csv # Summary of best-performing models
â”œâ”€â”€ requirements.txt # Python dependencies list
â”œâ”€â”€ .gitignore # Ignored files and system metadata
â””â”€â”€ creditcard.csv # Credit Card Fraud dataset (download manually from Kaggle)
```

---

## ğŸ§© Visualizations

Key plots generated under `/plots` include:
- ğŸ“ˆ **Classification Accuracy Heatmap**
- ğŸ“Š **F1-Score Comparison**
- ğŸ§® **Regression MSE Distribution**
- ğŸ” **Clustering Silhouette Score Visualization**

All visuals are compiled into [`visual.pdf`](visual.pdf).

---

## ğŸ§ª How to Run the Project

### 1ï¸âƒ£ Clone the repository
```bash
cd multi-dataset-ml-comparison
git clone https://github.com/NarendraM45/multi-dataset-ml-comparison.git
```

### 2ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```
### 3ï¸âƒ£Add the Kaggle dataset

Download the Credit Card Fraud dataset from:
ğŸ‘‰ https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud

and place it as creditcard.csv in the project folder.

### 4ï¸âƒ£ Run the Python script
```bash
python mulitidaatset_compariosn.py
```
### 5ï¸âƒ£ Explore results

# View:

Metrics in ml_results_full_2.csv

Graphs in the plots/ directory

Detailed analysis in visual.ipynb or visual.pdf

### ğŸ“‹ Requirements

Core dependencies are listed in requirements.txt.
Typical stack:

pandas
numpy
scikit-learn
matplotlib
seaborn
tqdm

### ğŸ§­ Key Findings

No single model dominates across all datasets.
Performance depends on dataset size, structure, and feature complexity.

Ensemble models (Random Forest, AdaBoost) show superior generalization.

Simple models like Logistic Regression and Linear Regression offer interpretability and speed.

Dataset-specific optimization (balancing, feature scaling, hyperparameter tuning) remains crucial.

### ğŸ§© Future Enhancements

Integration of deep learning architectures for image and text datasets

Automated hyperparameter optimization via GridSearchCV or Bayesian tuning

Explainable AI (XAI) metrics for interpretability

Inclusion of runtime and efficiency benchmarking

### ğŸ§¾ Citation & Credits

This project was developed as part of an academic machine learning study by Narendra Mishra
(3rd Year B.Tech, CSIT).

Dataset Credit:

Credit Card Fraud Detection Dataset:
MLG-ULB, Kaggle

### ğŸŒ Repository Link

ğŸ”— GitHub Repository:
https://github.com/NarendraM45/multi-dataset-ml-comparison

### ğŸ License

This project is released under the MIT License.
Feel free to fork, modify, and build upon it for research or learning purposes.
